{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scraping_tweets_for_AltasPilots.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvKbQrULd4la"
      },
      "source": [
        "# Scraping tweets from Twitter "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1ZyKojUSvQc",
        "outputId": "fe228dc2-1ee9-429e-96dd-8738aff4ad82"
      },
      "source": [
        "#mount to the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fotqe6HIAXmA"
      },
      "source": [
        "# make a folder to store data (You can update path here, tweets will be stored in here)\n",
        "# this is a place where tweets will be loaded into \n",
        "base_dir = '/content/drive/Shareddrives/OPIM 5770-Capstone Class for team 7/secondary_research /tweets from Altas Pilots'\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "#Create a new directory (a folder) in your Drive\n",
        "tf.io.gfile.mkdir(base_dir)\n",
        "\n",
        "#Combine the 3 strings\n",
        "gates_dir = base_dir + '/Atlas Air Pilots/'\n",
        "\n",
        "#Create the second directory\n",
        "tf.io.gfile.mkdir(gates_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1290mDyamteM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6097fdd-33c9-432a-a4f4-418148dcf0d7"
      },
      "source": [
        "# import snscrape to extract twittes \n",
        "!pip install snscrape\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snscrape\n",
            "  Downloading snscrape-0.3.4-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from snscrape) (4.6.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from snscrape) (4.2.6)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from snscrape) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (2021.10.8)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (1.7.1)\n",
            "Installing collected packages: snscrape\n",
            "Successfully installed snscrape-0.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE4jDRbS_VSZ"
      },
      "source": [
        "# not sure how many tweets this account has, but set to 7000 to collect all the tweets during this time of period\n",
        "maxTweets = 7000 \n",
        "\n",
        "# make sure you update the username and number of tweets!\n",
        "# time of perid: 2015-06-01 to 2021-06-30\n",
        "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:@AtlasAirPilots + since:2015-06-01 until:2021-06-30-filter:links -filter:replies').get_items()):\n",
        "  csvFile = open(gates_dir + str(i) + '.csv', 'a', newline='', encoding='utf8')\n",
        "  csvWriter = csv.writer(csvFile)\n",
        "  csvWriter.writerow(['id','date','tweet',])\n",
        "\n",
        "  if i > maxTweets :\n",
        "    break\n",
        "  csvWriter.writerow([tweet.id, tweet.date, tweet.content])\n",
        "csvFile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UCZaVSHqeQJ"
      },
      "source": [
        "# Done!"
      ]
    }
  ]
}